# å„translationãƒ†ãƒ¼ãƒ–ãƒ«ã«ï¼™è¨€èªåˆ†ã®QAã‚’å…¥ã‚Œã‚‹ã‚³ãƒ¼ãƒ‰ï¼ˆå…¨å‰Šé™¤â†’å†æŠ•å…¥ã¤ãï¼‰
import sqlite3
import time
import requests
from bs4 import BeautifulSoup
import re

# ===== è¨­å®š =====
DB_PATH   = "ShigaChat.db"   # ä¾‹: "app/ShigaChat.db" ãªã‚‰ã“ã“ã‚’å¤‰æ›´
BASE      = "https://www.s-i-a.or.jp"
FIXED_DT  = "2025-09-22 00:00:00"
UA        = {"User-Agent": "ShigaChatCrawler/1.0 (+https://example.com)"}
SLEEP_SEC = 0.7   # ã‚µã‚¤ãƒˆã«å„ªã—ã

# è¨€èªï¼ˆcode, lang_id, path_prefixï¼‰
LANGS = [
    ("ja",    1, ""),       # æ—¥æœ¬èªï¼ˆæ­£æº–ï¼‰
    ("en",    2, "/en"),
    ("vi",    3, "/vi"),
    ("zh-cn", 4, "/zh-cn"),
    ("ko",    5, "/ko"),
    ("pt",    6, "/pt"),
    ("es",    7, "/es"),
    ("tl",    8, "/tl"),
    ("id",    9, "/id"),
]

# ã‚«ãƒ†ã‚´ãƒªID â†’ ã‚¹ãƒ©ãƒƒã‚°
CATEGORY_SLUGS = {
    1: "immigration_residency_procedures",
    2: "daily_living",
    3: "medical_care",
    4: "pension_insurance",
    5: "labor",
    6: "education",
    7: "marriage_divorce",
    8: "childbirth_childcare",
    9: "housing",
    10: "tax",
    11: "welfare",
    12: "incidents_accidents",
    13: "disaster",
}

# ===== æ•´å½¢ï¼šHTMLâ†’è‡ªç„¶æ–‡ï¼ˆURLä¿æŒï¼‰ =====
async def html_to_plaintext(html: str) -> str:
    """
    å›ç­”HTMLã‚’è‡ªç„¶æ–‡ãƒ†ã‚­ã‚¹ãƒˆã«æ•´å½¢ã€‚
    - <a> ã¯ã€Œãƒ†ã‚­ã‚¹ãƒˆ (URL)ã€
    - <ol>/<ul> ã¯ç•ªå·/ç®‡æ¡æ›¸ã
    - <dl> ã¯ã€Œç”¨èªï¼šèª¬æ˜ã€
    - è¦‹å‡ºã—ã¯ã‚¿ã‚°é™¤å»ã—ã¦1è¡Œãƒ†ã‚­ã‚¹ãƒˆåŒ–
    - <p>, <br> ã¯æ”¹è¡Œ
    """
    if not html:
        return ""

    soup = BeautifulSoup(html, "html.parser")

    # ä¸è¦ã‚¿ã‚°é™¤å»
    for bad in soup(["script", "style"]):
        bad.decompose()

    # aã‚¿ã‚° => "ãƒ†ã‚­ã‚¹ãƒˆ (URL)"
    for a in soup.find_all("a"):
        txt = a.get_text(" ", strip=True)
        href = (a.get("href") or "").strip()
        a.replace_with(f"{txt} ({href})" if href else txt)

    # dl => ã€Œç”¨èªï¼šèª¬æ˜ã€
    for dl in soup.find_all("dl"):
        lines, term = [], None
        for child in dl.children:
            name = getattr(child, "name", None)
            if name == "dt":
                term = child.get_text(" ", strip=True)
            elif name == "dd":
                desc = child.get_text(" ", strip=True)
                if term:
                    lines.append(f"{term}ï¼š{desc}")
                    term = None
        dl.replace_with("\n".join(lines))

    # ol/ul => è¡Œã«å±•é–‹
    for lst in soup.find_all(["ol", "ul"]):
        items = []
        for i, li in enumerate(lst.find_all("li", recursive=False), start=1):
            txt = li.get_text(" ", strip=True)
            if not txt:
                continue
            items.append(f"{i}. {txt}" if lst.name == "ol" else f"- {txt}")
        lst.replace_with("\n".join(items))

    # è¦‹å‡ºã—
    for h in soup.find_all(["h1","h2","h3","h4","h5","h6"]):
        txt = h.get_text(" ", strip=True)
        h.replace_with(txt + "\n")

    # æ”¹è¡Œç³»
    for br in soup.find_all("br"):
        br.replace_with("\n")
    for p in soup.find_all("p"):
        p.replace_with(p.get_text(" ", strip=True) + "\n")

    text = soup.get_text("\n", strip=True)

    # ä½™åˆ†ãªæ”¹è¡Œãƒ»ç©ºç™½
    text = re.sub(r"[ \t]+\n", "\n", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    text = re.sub(r"[ \t]{2,}", " ", text)

    return text.strip()

# ===== DBãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
async def wipe_seed_tables(conn):
    """
    æ—¢å­˜ã®æŠ•å…¥æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿(question/answer/translation/QA)ã‚’ä¸€æ‹¬å‰Šé™¤ã—ã€
    AUTOINCREMENTã®æ¡ç•ªã‚‚ãƒªã‚»ãƒƒãƒˆã™ã‚‹ã€‚
    """
    cur = conn.cursor()
    cur.execute("PRAGMA foreign_keys=OFF;")
    cur.execute("BEGIN;")
    try:
        # ä¾å­˜ã®å¼±ã„é †ã«å‰Šé™¤
        cur.execute("DELETE FROM QA;")
        cur.execute("DELETE FROM question_translation;")
        cur.execute("DELETE FROM answer_translation;")
        cur.execute("DELETE FROM question;")
        cur.execute("DELETE FROM answer;")

        # æ¡ç•ªãƒªã‚»ãƒƒãƒˆï¼ˆå­˜åœ¨ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã ã‘å¯¾è±¡ã«ãªã‚‹ã®ã§å®‰å…¨ï¼‰
        cur.execute("""
            DELETE FROM sqlite_sequence
             WHERE name IN ('QA','question_translation','answer_translation','question','answer');
        """)

        cur.execute("COMMIT;")
    except Exception:
        cur.execute("ROLLBACK;")
        raise
    finally:
        cur.execute("PRAGMA foreign_keys=ON;")

async def ensure_user(cur):
    cur.execute("SELECT id FROM user WHERE name=?", ("sia",))
    row = cur.fetchone()
    if row:
        return row[0]
    cur.execute("INSERT INTO user (name, password) VALUES (?, ?)", ("sia", "sia"))
    return cur.lastrowid

async def insert_question(cur, category_id, user_id):
    cur.execute("""
        INSERT INTO question (category_id, time, language_id, user_id, title, content, public)
        VALUES (?, ?, ?, ?, ?, ?, 1)
    """, (category_id, FIXED_DT, 1, user_id, "Untitled", "No content"))
    return cur.lastrowid

# Answerã¯Qã”ã¨ã«1ä»¶ã ã‘ä½œæˆï¼ˆlanguage_idã¯JA=1ã§å›ºå®šï¼‰
async def insert_answer(cur):
    cur.execute("INSERT INTO answer (time, language_id) VALUES (?, ?)", (FIXED_DT, 1))
    return cur.lastrowid

async def link_QA(cur, qid, aid):
    cur.execute("INSERT INTO QA (question_id, answer_id) VALUES (?, ?)", (qid, aid))

async def insert_q_trans(cur, qid, lang_id, text):
    cur.execute("""
        INSERT INTO question_translation (question_id, language_id, texts, checked)
        VALUES (?, ?, ?, 1)
    """, (qid, lang_id, text or ""))

async def insert_a_trans(cur, aid, lang_id, text_plain):
    cur.execute("""
        INSERT INTO answer_translation (answer_id, language_id, texts, checked)
        VALUES (?, ?, ?, 1)
    """, (aid, lang_id, text_plain or ""))

# ===== å–å¾—ãƒ»è§£æ =====
async def fetch_soup(url):
    r = requests.get(url, headers=UA, timeout=30)
    r.raise_for_status()
    return BeautifulSoup(r.text, "html.parser")

async def extract_pairs(soup):
    """å„ paragraph--type--consulting-qa ã‹ã‚‰ (Q_text, A_html) ã®é…åˆ—ã‚’è¿”ã™"""
    pairs = []
    for blk in soup.select(".paragraph--type--consulting-qa"):
        q = blk.select_one(".field--name-field-question .field__item")
        a = blk.select_one(".field--name-field-answer  .field__item")
        if not (q and a):
            continue
        q_text = q.get_text(" ", strip=True)
        a_html = a.decode_contents()  # è¦‹å‡ºã—/ãƒªãƒ³ã‚¯ä¿æŒ
        pairs.append((q_text, a_html))
    return pairs

async def load_category_all_lang(slug):
    """1ã‚«ãƒ†ã‚´ãƒªã®å…¨è¨€èªãƒšãƒ¼ã‚¸ã‚’1å›ãšã¤å–å¾—ã—ã¦è¾æ›¸ã§è¿”ã™: {lang_id: [(Q, A_html), ...]}"""
    result = {}
    for code, lang_id, prefix in LANGS:
        url = f"{BASE}{prefix}/qa/{slug}" if code != "ja" else f"{BASE}/qa/{slug}"
        print("GET", url)
        try:
            soup = fetch_soup(url)
            pairs = extract_pairs(soup)
            result[lang_id] = pairs
        except Exception as e:
            print(f"  !! fetch error ({code}): {e}")
            result[lang_id] = []
        time.sleep(SLEEP_SEC)
    return result

# ===== ãƒ¡ã‚¤ãƒ³å‡¦ç† =====
async def main():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()

    # â˜… 1) æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªã‚¢
    print("Wiping existing QA/translation data...")
    wipe_seed_tables(conn)

    # â˜… 2) user ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ç¢ºä¿
    user_id = ensure_user(cur)

    # â˜… 3) åé›† & ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
    for category_id, slug in CATEGORY_SLUGS.items():
        print(f"\n==== Category {category_id}: {slug} ====")
        by_lang = load_category_all_lang(slug)

        base = by_lang.get(1, [])  # æ—¥æœ¬èªã‚’æ­£æº–
        if not base:
            print("  !! æ—¥æœ¬èªãƒšãƒ¼ã‚¸ã§Q/AãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—")
            continue

        # å„Q/Aï¼ˆæ—¥æœ¬èªã®ä»¶æ•°ã«åˆã‚ã›ã‚‹ï¼‰
        for idx, (q_ja, a_ja) in enumerate(base, start=1):
            # 1) questionï¼ˆJAåŸºæº–ã§1ä»¶ä½œæˆï¼‰
            qid = insert_question(cur, category_id, user_id)

            # 2) answer ã¯ Q ã”ã¨ã« 1 ä»¶ã ã‘ä½œæˆ
            aid = insert_answer(cur)

            # 3) å„è¨€èªã® ç¿»è¨³(question_translation, answer_translation) ã‚’ã¶ã‚‰ä¸‹ã’ã‚‹
            for code, lang_id, _ in LANGS:
                # è¨€èªã”ã¨ã®(Q, A)å–å¾—ï¼ˆç„¡ã‘ã‚Œã°ç©ºæ–‡å­—ï¼‰
                q_text = ""
                a_html = ""
                pairs = by_lang.get(lang_id, [])
                if idx - 1 < len(pairs):
                    q_text, a_html = pairs[idx - 1]

                # è³ªå•ç¿»è¨³ï¼ˆãƒ—ãƒ¬ãƒ¼ãƒ³ï¼‰
                insert_q_trans(cur, qid, lang_id, q_text)

                # å›ç­”ç¿»è¨³ï¼šHTMLâ†’è‡ªç„¶æ–‡ï¼ˆURLä¿æŒï¼‰
                a_text_plain = html_to_plaintext(a_html)
                insert_a_trans(cur, aid, lang_id, a_text_plain)

            # 4) QAãƒªãƒ³ã‚¯ã¯1å›ã ã‘
            link_QA(cur, qid, aid)

            if idx % 5 == 0:
                conn.commit()
                print(f"  ... inserted {idx} QAs")

        conn.commit()
        print(f"âœ… Category {category_id} done: {len(base)} base QAs")

    # ï¼ˆä»»æ„ï¼‰DBã®ç©ºãé ˜åŸŸã‚’ç¸®å°ã—ãŸã„å ´åˆ
    try:
        conn.execute("VACUUM;")
    except Exception:
        pass

    conn.close()
    print("\nğŸ‰ All done.")

if __name__ == "__main__":
    main()

#Question tableã«contentã‚’å…¥ã‚Œã‚‹ã‚³ãƒ¼ãƒ‰
import requests
from bs4 import BeautifulSoup
import sqlite3
from datetime import datetime

DB_PATH = "ShigaChat.db"
BASE_URL = "https://www.s-i-a.or.jp/qa"

# ã‚«ãƒ†ã‚´ãƒªID â†’ ã‚¹ãƒ©ãƒƒã‚°ï¼ˆæ—¥æœ¬èªãƒšãƒ¼ã‚¸ã®ã¿ä½¿ç”¨ï¼‰
CATEGORIES = {
    1: "immigration_residency_procedures",
    2: "daily_living",
    3: "medical_care",
    4: "pension_insurance",
    5: "labor",
    6: "education",
    7: "marriage_divorce",
    8: "childbirth_childcare",
    9: "housing",
    10: "tax",
    11: "welfare",
    12: "incidents_accidents",
    13: "disaster",
}

FIXED_DATE = "2025-09-22 00:00:00"
EDITOR_ID = 1

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; ShigaChatScraper/1.0; +https://example.com)"
}

async def fetch_questions_ja(url: str):
    """æ—¥æœ¬èªãƒšãƒ¼ã‚¸ã‹ã‚‰è³ªå•æ–‡ã ã‘ã‚’æŠ½å‡º"""
    r = requests.get(url, headers=HEADERS, timeout=30)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")

    # æ­£å¼ãªæ§‹é€ ï¼šè³ªå•ã¯ .field--name-field-question .field__item
    nodes = soup.select(".field--name-field-question .field__item")
    questions = []
    for n in nodes:
        text = n.get_text(separator=" ", strip=True)
        if text:
            questions.append(text)
    return questions

async def main():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()

    total_updated = 0

    for cat_id, slug in CATEGORIES.items():
        url = f"{BASE_URL}/{slug}"
        print(f"==== Category {cat_id}: {slug} ====")

        questions = fetch_questions_ja(url)
        print(f"  scraped {len(questions)} question(s)")

        if not questions:
            print("  âš  è³ªå•ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚»ãƒ¬ã‚¯ã‚¿å¤‰æ›´ã®å¯èƒ½æ€§ï¼‰")
            continue

        # DBå´ï¼šã“ã®ã‚«ãƒ†ã‚´ãƒªã® question_id ã‚’æ˜‡é †ã§å–å¾—
        cur.execute(
            "SELECT question_id FROM question WHERE category_id=? ORDER BY question_id ASC",
            (cat_id,),
        )
        qids = [row[0] for row in cur.fetchall()]

        if not qids:
            print("  âš  ã“ã®ã‚«ãƒ†ã‚´ãƒªã® question è¡ŒãŒDBã«ã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã« question ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
            continue

        # å–å¾—æ•°ã«åˆã‚ã›ã¦æ›´æ–°ï¼ˆä¸è¶³åˆ†/è¶…éåˆ†ã¯è­¦å‘Šï¼‰
        n_update = min(len(questions), len(qids))
        if len(questions) != len(qids):
            print(f"  â„¹ ä»¶æ•°å·®ã‚ã‚Šï¼šscraped={len(questions)}, db_rows={len(qids)} â†’ {n_update}ä»¶ã ã‘æ›´æ–°ã—ã¾ã™")

        for i in range(n_update):
            qid = qids[i]
            q_text = questions[i]
            cur.execute(
                """
                UPDATE question
                   SET content = ?, last_editor_id = ?, last_edited_at = ?
                 WHERE question_id = ?
                """,
                (q_text, EDITOR_ID, FIXED_DATE, qid),
            )

        conn.commit()
        total_updated += n_update
        print(f"  âœ… updated {n_update} row(s) for category {cat_id}")

    conn.close()
    print(f"\nğŸ‰ Done. Total updated rows: {total_updated}")

if __name__ == "__main__":
    main()
