import sqlite3
import pickle
from langdetect import detect
from openai import OpenAI
import faiss
import numpy as np
from collections import defaultdict
import dotenv
import os
from tqdm import tqdm
from langchain.chat_models import ChatOpenAI
dotenv.load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)
DB_PATH = "../../ShigaChat.db"  # SQLiteãƒ•ã‚¡ã‚¤ãƒ«å
# è¨€èªID â†’ è¨€èªã‚³ãƒ¼ãƒ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’DBã‹ã‚‰å–å¾—
def get_language_map():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("SELECT id, code FROM language")
    rows = cur.fetchall()
    conn.close()
    return {row[0]: row[1].lower() for row in rows}
# OpenAIãƒ™ã‚¯ãƒˆãƒ«å–å¾—
def get_embedding(text):
    response = client.embeddings.create(
        input=[text],
        model="text-embedding-3-small"
    )
    return response.data[0].embedding
# 1. ãƒ™ã‚¯ãƒˆãƒ«äº‹å‰ä½œæˆãƒ•ã‚§ãƒ¼ã‚º
def generate_and_save_vectors():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    print("ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆé–‹å§‹...")
    # å…¨QAãƒšã‚¢ã‚’å–å¾—
    cur.execute("SELECT id, question_id, answer_id FROM QA")
    qa_rows = cur.fetchall()
    print(f"ç·QAæ•°: {len(qa_rows)} ä»¶")
    LANGUAGE_MAP = get_language_map()
    print(f"å¯¾å¿œè¨€èª: {list(LANGUAGE_MAP.values())}")
    lang_text_map = defaultdict(list)  # è¨€èªã”ã¨ã® [(embedding, meta, text)] æ ¼ç´
    for qa_id, qid, aid in tqdm(qa_rows, desc="ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆä¸­"):
        for lang_id, lang_code in LANGUAGE_MAP.items():
            cur.execute("SELECT texts FROM question_translation WHERE question_id=? AND language_id=?", (qid, lang_id))
            qrow = cur.fetchone()
            cur.execute("SELECT texts FROM answer_translation WHERE answer_id=? AND language_id=?", (aid, lang_id))
            arow = cur.fetchone()
            cur.execute("SELECT time FROM question WHERE question_id=?", (qid,))
            trow = cur.fetchone()
            if qrow and arow and trow:
                question_text = qrow[0]
                answer_text = arow[0]
                time_val = trow[0]
                text = f"Q: {question_text}\nA: {answer_text}"
                embedding = get_embedding(text)
                lang_text_map[lang_code].append(
                    (embedding, (qa_id, qid), (question_text, answer_text, time_val))
                )
    for lang_code, data in lang_text_map.items():
        if not data:
            print(f"{lang_code} ã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºãªã®ã§ã‚¹ã‚­ãƒƒãƒ—")
            continue
        vectors = np.array([x[0] for x in data]).astype("float32")
        meta = [x[1] for x in data]   # (qa_id, question_id)
        texts = [x[2] for x in data]  # (question_text, answer_text, time)
        index = faiss.IndexFlatL2(vectors.shape[1])
        index.add(vectors)
        VEC_DIR = os.path.dirname(os.path.abspath(DB_PATH))
        vec_file = os.path.join(VEC_DIR, f"vectors_{lang_code}.pkl")
        with open(vec_file, "wb") as f:
            pickle.dump((index, meta, texts), f)
        print(f"ä¿å­˜å®Œäº†: {vec_file}ï¼ˆ{len(data)} ä»¶ï¼‰")
    conn.close()
    print(":ãƒã‚§ãƒƒã‚¯ãƒãƒ¼ã‚¯_ç·‘: å…¨ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜å®Œäº†")
# 2. RAGæ¤œç´¢é–¢æ•°ï¼ˆDBå‚ç…§ãªã—ï¼‰
def rag(question):
    lang = detect(question)
    print(f"æ¤œå‡ºè¨€èª: {lang}")
    vec_path = f"../../vectors_{lang}.pkl"
    if not os.path.exists(vec_path):
        print(f":å°åŒ…ã¿: ãƒ™ã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {vec_path} â†’ ç”Ÿæˆã‚’è©¦ã¿ã¾ã™")
        generate_and_save_vectors()
    if not os.path.exists(vec_path):
        raise Exception(f":x: ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆç”Ÿæˆã«ã‚‚å¤±æ•—ï¼‰: {vec_path}")
    with open(vec_path, "rb") as f:
        index, meta, texts = pickle.load(f)
    query_vec = np.array(get_embedding(question)).astype("float32").reshape(1, -1)
    D, I = index.search(query_vec, 5)
    results = {}
    for rank, idx in enumerate(I[0]):
        question_text, answer_text, time_val = texts[idx]
        results[rank + 1] = [answer_text, question_text, time_val]
    return results

# LLMã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆ
def generate_answer_with_llm(question_text: str, rag_qa: dict, history_qa: list) -> str:
    prompt = "ã‚ãªãŸã¯æ»‹è³€çœŒã«ä½ã‚€å¤–å›½äººã«æƒ…å ±ã‚’æä¾›ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚\n"
    prompt += "ä»¥ä¸‹ã¯å‚è€ƒæƒ…å ±ã§ã™:\n\n"

    # ğŸ”¹ RAGã‹ã‚‰ã®é–¢é€£QA
    prompt += "ã€RAGã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸQAã€‘\n"
    for i, (_, qa_list) in enumerate(rag_qa.items(), 1):
        q, a, t = qa_list
        prompt += f"Q{i}: {q}\nA{i}: {a}\n"

    # ğŸ”¹ éå»ã®å¯¾è©±å±¥æ­´
    prompt += "\nã€ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã€‘\n"
    for i, (q, a) in enumerate(history_qa, 1):
        prompt += f"User{i}: {q}\nBot{i}: {a}\n"

    # ğŸ”¹ æ–°ã—ã„è³ªå•
    prompt += f"\nã€ç¾åœ¨ã®è³ªå•ã€‘\n{question_text}\n"
    prompt += "\nã“ã®è³ªå•ã«å¯¾ã—ã¦ã€å‚è€ƒæƒ…å ±ã¨ä¼šè©±å±¥æ­´ã‚’è¸ã¾ãˆã¦é©åˆ‡ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"

    # ğŸ”¹ ChatOpenAIå‘¼ã³å‡ºã—
    client = ChatOpenAI(api_key=OPENAI_API_KEY)
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
    )
    return response.choices[0].message.content.strip()